{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b21d2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 07:58:41 INFO: Loading these models for language: grc (Ancient_Greek):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | perseus          |\n",
      "| pos       | perseus_nocharlm |\n",
      "| lemma     | perseus_nocharlm |\n",
      "================================\n",
      "\n",
      "2025-03-14 07:58:41 WARNING: GPU requested, but is not available!\n",
      "2025-03-14 07:58:41 INFO: Using device: cpu\n",
      "2025-03-14 07:58:41 INFO: Loading: tokenize\n",
      "2025-03-14 07:58:41 INFO: Loading: pos\n",
      "2025-03-14 07:58:41 INFO: Loading: lemma\n",
      "2025-03-14 07:58:41 INFO: Done loading processors!\n",
      "2025-03-14 07:58:41 INFO: Loading these models for language: grc (Ancient_Greek):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | proiel          |\n",
      "| pos       | proiel_nocharlm |\n",
      "| lemma     | proiel_nocharlm |\n",
      "===============================\n",
      "\n",
      "2025-03-14 07:58:41 WARNING: GPU requested, but is not available!\n",
      "2025-03-14 07:58:41 INFO: Using device: cpu\n",
      "2025-03-14 07:58:41 INFO: Loading: tokenize\n",
      "2025-03-14 07:58:41 INFO: Loading: pos\n",
      "2025-03-14 07:58:41 INFO: Loading: lemma\n",
      "2025-03-14 07:58:41 INFO: Done loading processors!\n",
      "2025-03-14 07:58:41 INFO: Loading these models for language: el (Greek):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gdt          |\n",
      "| mwt       | gdt          |\n",
      "| pos       | gdt_nocharlm |\n",
      "| lemma     | gdt_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-03-14 07:58:41 WARNING: GPU requested, but is not available!\n",
      "2025-03-14 07:58:41 INFO: Using device: cpu\n",
      "2025-03-14 07:58:41 INFO: Loading: tokenize\n",
      "2025-03-14 07:58:41 INFO: Loading: mwt\n",
      "2025-03-14 07:58:41 INFO: Loading: pos\n",
      "2025-03-14 07:58:41 INFO: Loading: lemma\n",
      "2025-03-14 07:58:41 INFO: Done loading processors!\n",
      "2025-03-14 07:58:41 INFO: Loading these models for language: el (Greek):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gud          |\n",
      "| pos       | gud_nocharlm |\n",
      "| lemma     | gud_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-03-14 07:58:41 WARNING: GPU requested, but is not available!\n",
      "2025-03-14 07:58:41 INFO: Using device: cpu\n",
      "2025-03-14 07:58:41 INFO: Loading: tokenize\n",
      "2025-03-14 07:58:41 INFO: Loading: pos\n",
      "2025-03-14 07:58:42 INFO: Loading: lemma\n",
      "2025-03-14 07:58:42 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "input_folder = 'example'\n",
    "output_folders = {\n",
    "    'ancient-greek-proiel': 'exam_lemmatized_stanza_ag',\n",
    "    'ancient-greek-perseus': 'exam_lemmatized_stanza_agp',\n",
    "    'greek-1': 'exam_lemmatized_stanza_g_1',\n",
    "    'greek-2': 'exam_lemmatized_stanza_g_2',\n",
    "    'all': 'exam_lemmatized_stanza_all'\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'ancient-greek-perseus': stanza.Pipeline(\n",
    "        lang = 'grc',\n",
    "        processors='tokenize, pos, lemma',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    ),\n",
    "    'ancient-greek-proiel': stanza.Pipeline(\n",
    "        lang = 'grc', \n",
    "        processors='tokenize, pos, lemma', \n",
    "        package = 'proiel',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    ),\n",
    "    'greek-1': stanza.Pipeline(\n",
    "        lang = 'el', \n",
    "        processors='tokenize, mwt, pos, lemma',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    ),\n",
    "    'greek-2': stanza.Pipeline(\n",
    "        lang = 'el', \n",
    "        processors='tokenize, pos, lemma', \n",
    "        package = 'gud',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e04ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun = ['με', 'μας', 'σε', 'σας', 'τον', 'τους', 'την', 'τις', 'το', 'τα', 'μου', 'μας', 'σου', 'σας', 'του', 'τους',\n",
    "        'της', 'τους', 'του', 'τους', 'εγώ', 'εμείς', 'εσύ', 'εσείς', 'αυτός', 'αυτοί', 'αυτή', 'αυτές', 'αυто', 'αυτά']\n",
    "numerals = [\n",
    "    \"ένα\", \"δύο\", \"τρία\", \"τέσσερα\", \"πέντε\", \"έξι\", \"επτά\", \"οκτώ\", \"εννέα\", \"δέκα\", \"είκοσι\", \"τριάντα\",\n",
    "    \"σαράντα\", \"πενήντα\", \"εξήντα\", \"εβδομήντα\", \"ογδόντа\", \"ενενήντα\", \"εκατό\", \"διακόσια\", \"τριακόσια\",\n",
    "    \"τετρακόσια\", \"πεντακόσια\", \"εξακόσια\", \"επτακόσια\", \"οκτακόσια\", \"εννιακόσια\", \"χίλια\", \"δέκα χιλιάδες\",\n",
    "    \"εκατό χιλιάδες\", \"ένα εκατομμύριο\"\n",
    "]\n",
    "words_to_remove = noun + numerals\n",
    "\n",
    "tonos_replacements = {\n",
    "        'ά': 'α',\n",
    "        'έ': 'ε',\n",
    "        'ή': 'η',\n",
    "        'ί': 'ι',\n",
    "        'ό': 'ο',\n",
    "        'ύ': 'υ',\n",
    "        'ώ': 'ω',\n",
    "        'Ά': 'Α',\n",
    "        'Έ': 'Ε',\n",
    "        'Ή': 'Η',\n",
    "        'Ί': 'Ι',\n",
    "        'Ό': 'Ο',\n",
    "        'Ύ': 'Υ',\n",
    "        'Ώ': 'Ω'}\n",
    "\n",
    "def remove_tonos_greek(text):\n",
    "    result = text\n",
    "    for tonos_char, plain_char in tonos_replacements.items():\n",
    "        result = result.replace(tonos_char, plain_char)\n",
    "    \n",
    "    return result\n",
    "\n",
    "for i in range(len(words_to_remove)):\n",
    "    words_to_remove[i] = remove_tonos_greek(words_to_remove[i])\n",
    "    \n",
    "label_dict = {'PROPN': 'person1', 'PRON': 'pron1', 'NUM': 'ordinal1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8584b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = remove_tonos_greek(re.sub(r'[^α-ωΑ-Ω0-9\\s\\.,;:!?\\(\\)\\[\\]\\{\\}\\'\"«»\\-]', '', text))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a38028fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_with_stanza(text, pipeline):\n",
    "    processed_text = pipeline(text)\n",
    "    prepared_text = []\n",
    "    \n",
    "    for sentence in processed_text.sentences:\n",
    "        for word in sentence.words:\n",
    "            upos = word.upos\n",
    "            \n",
    "            if upos is None:\n",
    "                continue\n",
    "            \n",
    "            if upos in label_dict:\n",
    "                prepared_text.append(label_dict[upos])\n",
    "            elif upos != 'PUNCT':\n",
    "                lemma = word.lemma.lower() if word.lemma else word.text.lower()\n",
    "                if lemma not in words_to_remove:\n",
    "                    prepared_text.append(lemma)\n",
    "    \n",
    "    return ' '.join(prepared_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bacd390",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        text = remove_tonos_greek(text)\n",
    "        text = clean_text(text)\n",
    "        \n",
    "        for lang_code, pipeline in pipelines.items():\n",
    "            processed_text = process_text_with_stanza(text, pipeline)\n",
    "            \n",
    "            output_path = os.path.join(output_folders[lang_code], filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(processed_text)\n",
    "        \n",
    "        combined_text = text\n",
    "        for pipeline in pipelines.values():\n",
    "            combined_text = process_text_with_stanza(combined_text, pipeline)\n",
    "        \n",
    "        combined_output_path = os.path.join(output_folders['all'], filename)\n",
    "        with open(combined_output_path, 'w', encoding='utf-8') as combined_output_file:\n",
    "            combined_output_file.write(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b56530f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'PROPN': 'person1', 'PRON': 'pron1', 'NUM': 'ordinal1'}\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def load_documents_in_batches(input_folder, batch_size=100):\n",
    "    filenames = [filename for filename in os.listdir(input_folder) if filename.endswith('.txt')]\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch_filenames = filenames[i:i + batch_size]\n",
    "        documents = []\n",
    "        for filename in batch_filenames:\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            with open(input_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read().strip()\n",
    "                text = remove_tonos_greek(text)\n",
    "                text = clean_text(text)\n",
    "            documents.append(stanza.Document([], text=text))\n",
    "        batches.append((documents, batch_filenames))\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def process_documents(documents, pipeline):\n",
    "    return pipeline(documents)\n",
    "\n",
    "def extract_lemmas(doc):\n",
    "    current_result = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            upos = word.upos\n",
    "            \n",
    "            if upos is None:\n",
    "                continue\n",
    "            \n",
    "            if upos in label_dict:\n",
    "                current_result.append(label_dict[upos])\n",
    "            elif upos != 'PUNCT':\n",
    "                lemma = word.lemma.lower() if word.lemma else word.text.lower()\n",
    "                if lemma not in words_to_remove:\n",
    "                    current_result.append(lemma)\n",
    "    return ' '.join(current_result)\n",
    "\n",
    "def process_all_documents_in_batches(batch_size=100):\n",
    "    batches = load_documents_in_batches(input_folder, batch_size=batch_size)\n",
    "    \n",
    "    for documents, filenames in batches:\n",
    "        processed_results = {lang_code: [] for lang_code in pipelines.keys()}\n",
    "        for lang_code, pipeline in pipelines.items():\n",
    "            out_docs = process_documents(documents, pipeline)\n",
    "            processed_results[lang_code] = out_docs\n",
    "        save_results(processed_results, filenames, output_folders)\n",
    "\n",
    "def save_results(processed_results, filenames, output_folders):\n",
    "    for lang_code, out_docs in processed_results.items():\n",
    "        output_folder = output_folders[lang_code]\n",
    "\n",
    "        for i, doc in enumerate(out_docs):\n",
    "            processed_text = extract_lemmas(doc)\n",
    "            output_path = os.path.join(output_folder, filenames[i])\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(processed_text)\n",
    "\n",
    "process_all_documents_in_batches(batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7549bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
