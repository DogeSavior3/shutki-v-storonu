{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e31f6acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained XLM-Roberta, this may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/adapters/composition.py:225: FutureWarning: Passing list objects for adapter activation is deprecated. Please use Stack or Fuse explicitly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for ancient-greek\n",
      "Loading tagger for ancient-greek\n",
      "Loading lemmatizer for ancient-greek\n",
      "==================================================\n",
      "Active language: ancient-greek\n",
      "==================================================\n",
      "Loading pretrained XLM-Roberta, this may take a while...\n",
      "Loading tokenizer for ancient-greek-perseus\n",
      "Loading tagger for ancient-greek-perseus\n",
      "Loading lemmatizer for ancient-greek-perseus\n",
      "==================================================\n",
      "Active language: ancient-greek-perseus\n",
      "==================================================\n",
      "Loading pretrained XLM-Roberta, this may take a while...\n",
      "Loading tokenizer for greek\n",
      "Loading tagger for greek\n",
      "Loading multi-word expander for greek\n",
      "Loading lemmatizer for greek\n",
      "==================================================\n",
      "Active language: greek\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from trankit import Pipeline\n",
    "\n",
    "input_folder = 'example'\n",
    "output_folders = {\n",
    "    'ancient-greek': 'lemmatized_trankit_ag',\n",
    "    'ancient-greek-perseus': 'lemmatized_trankit_agp',\n",
    "    'greek': 'lemmatized_trankit_g',\n",
    "    'all': 'lemmatized_trankit_all'\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'ancient-greek': Pipeline('ancient-greek'),\n",
    "    'ancient-greek-perseus': Pipeline('ancient-greek-perseus'),\n",
    "    'greek': Pipeline('greek')\n",
    "}\n",
    "\n",
    "noun = ['με', 'μας', 'σε', 'σας', 'τον', 'τους', 'την', 'τις', 'το', 'τα', 'μου', 'μας', 'σου', 'σας', 'του', 'τους',\n",
    "        'της', 'τους', 'του', 'τους', 'εγώ', 'εμείς', 'εσύ', 'εσείς', 'αυτός', 'αυτοί', 'αυτή', 'αυτές', 'αυто', 'αυτά']\n",
    "numerals = [\n",
    "    \"ένα\", \"δύο\", \"τρία\", \"τέσσερα\", \"πέντε\", \"έξι\", \"επτά\", \"οκτώ\", \"εννέα\", \"δέκα\", \"είκοσι\", \"τριάντα\",\n",
    "    \"σαράντα\", \"πενήντα\", \"εξήντα\", \"εβδομήντα\", \"ογδόντа\", \"ενενήντα\", \"εκατό\", \"διακόσια\", \"τριακόσια\",\n",
    "    \"τετρακόσια\", \"πεντακόσια\", \"εξακόσια\", \"επτακόσια\", \"οκτακόσια\", \"εννιακόσια\", \"χίλια\", \"δέκα χιλιάδες\",\n",
    "    \"εκατό χιλιάδες\", \"ένα εκατομμύριο\"\n",
    "]\n",
    "words_to_remove = noun + numerals\n",
    "\n",
    "tonos_replacements = {\n",
    "        'ά': 'α',\n",
    "        'έ': 'ε',\n",
    "        'ή': 'η',\n",
    "        'ί': 'ι',\n",
    "        'ό': 'ο',\n",
    "        'ύ': 'υ',\n",
    "        'ώ': 'ω',\n",
    "        'Ά': 'Α',\n",
    "        'Έ': 'Ε',\n",
    "        'Ή': 'Η',\n",
    "        'Ί': 'Ι',\n",
    "        'Ό': 'Ο',\n",
    "        'Ύ': 'Υ',\n",
    "        'Ώ': 'Ω'}\n",
    "\n",
    "def remove_tonos_greek(text):\n",
    "    result = text\n",
    "    for tonos_char, plain_char in tonos_replacements.items():\n",
    "        result = result.replace(tonos_char, plain_char)\n",
    "    \n",
    "    return result\n",
    "\n",
    "for i in range(len(words_to_remove)):\n",
    "    words_to_remove[i] = remove_tonos_greek(words_to_remove[i])\n",
    "    \n",
    "label_dict = {'PROPN': 'person1', 'PRON': 'pron1', 'NUM': 'ordinal1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f0e167-311a-4162-9f86-bda166fc51f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизация и обработка завершены.\n"
     ]
    }
   ],
   "source": [
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^α-ωΑ-Ω0-9\\s\\.,;:!?\\(\\)\\[\\]\\{\\}\\'\"«»\\-]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_words_to_remove(text, words_to_remove):\n",
    "    return ' '.join(word for word in text.split() if word not in words_to_remove)\n",
    "\n",
    "def process_text_with_trankit(text, pipeline):\n",
    "    processed_text = pipeline(text)\n",
    "    prepared_text = []\n",
    "    \n",
    "    for sentence in processed_text['sentences']:\n",
    "        for token in sentence['tokens']:\n",
    "            upos = token.get('upos', None)  \n",
    "            if upos is None:\n",
    "                continue\n",
    "\n",
    "            if upos in label_dict:\n",
    "                prepared_text.append(label_dict[upos])\n",
    "            elif upos != 'PUNCT':  \n",
    "                lemma = token.get('lemma', token['text']).lower()\n",
    "                if lemma not in words_to_remove:\n",
    "                    prepared_text.append(lemma)\n",
    "    \n",
    "    return ' '.join(prepared_text)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        text = remove_tonos_greek(text)\n",
    "        text = clean_text(text)\n",
    "        \n",
    "        for lang_code, pipeline in pipelines.items():\n",
    "            processed_text = process_text_with_trankit(text, pipeline)\n",
    "            \n",
    "            output_path = os.path.join(output_folders[lang_code], filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(processed_text)\n",
    "        \n",
    "        combined_text = text\n",
    "        for pipeline in pipelines.values():\n",
    "            combined_text = process_text_with_trankit(combined_text, pipeline)\n",
    "        \n",
    "        combined_output_path = os.path.join(output_folders['all'], filename)\n",
    "        with open(combined_output_path, 'w', encoding='utf-8') as combined_output_file:\n",
    "            combined_output_file.write(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5151f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
