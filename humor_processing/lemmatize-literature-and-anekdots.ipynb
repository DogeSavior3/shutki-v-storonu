{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a94fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27e8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_excel('ru_texts.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4902bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy(line):\n",
    "    line = line.lower().strip()\n",
    "    line = re.sub(r'<(.*?)>',' ',line)\n",
    "    line = re.sub(r'[^\\u0400-\\u04FF\\s.,!?]', '', line) \n",
    "    line = re.sub(r'[.]{1}','',line)\n",
    "    line = re.sub(r'[\\d]', '', line)\n",
    "    line = re.sub(r'[a-zA-Z]', '', line)\n",
    "    line = line.replace('\"','').replace('“','').replace('”','').replace('’','').replace('‘','').replace('—','').replace('-','').replace(':', '').replace(';','').replace('(', '').replace(')', '').replace('»', '').replace('«', '').replace('*','')\n",
    "    if len(line) != 0:\n",
    "        return(line.strip())\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5656d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.read_excel('ru_texts.xlsx')\n",
    "literatura = df_text['text'].tolist()\n",
    "\n",
    "with open('pure_anekdots.txt', 'r', encoding='utf-8') as f:\n",
    "    anekdoty = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1ed6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "anekdoty_blocks = [i for i in open('pure_anekdots.txt', 'r', encoding = 'utf-8').read().split('\\n\\n') if i != '']\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "with open('anekdots_trunc.txt', 'w', encoding='utf-8') as f:\n",
    "    for block in anekdoty_blocks:\n",
    "        processed_block = prepare_russian_text_without_file(block, segmenter, emb, morph_tagger, ner_tagger, morph_vocab)\n",
    "        f.write(processed_block + '\\n\\n\\n')\n",
    "\n",
    "# with open('pure_literature.txt', 'w', encoding='utf-8') as f:\n",
    "#     for text in literatura:\n",
    "#         f.write(str(text) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d22f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter, MorphVocab,\n",
    "    NewsNERTagger,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "import re\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6530a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_russian_text_without_file(raw_text, segmenter, emb, morph_tagger, ner_tagger, morph_vocab):    \n",
    "    label_dict = {'NUM': '0 ', 'PRON': '1 ', 'PER': '2 ', 'LOC': '3 ', 'ORG': '4 '}\n",
    "    next_label_num = 5\n",
    "\n",
    "    raw_text = re.sub(r'\\d+', '0' , raw_text)\n",
    "\n",
    "    doc = Doc(raw_text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "\n",
    "    for span in reversed(doc.ner.spans):\n",
    "        if span.type not in label_dict:\n",
    "            label_dict[span.type] = str(next_label_num)\n",
    "            next_label_num += 1\n",
    "        raw_text = \"\".join((raw_text[:span.start], label_dict[span.type], raw_text[span.stop:]))\n",
    "\n",
    "    doc = Doc(raw_text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "\n",
    "    prepared_text = ''\n",
    "    for token in doc.tokens:\n",
    "        if token.pos in label_dict:\n",
    "            prepared_text = ''.join([prepared_text, label_dict[token.pos]])\n",
    "\n",
    "        elif token.pos != 'PUNCT':\n",
    "            try:\n",
    "                token.lemmatize(morph_vocab)\n",
    "                prepared_text = ''.join([prepared_text, token.lemma.lower(), ' '])\n",
    "            except Exception as ex:\n",
    "                prepared_text = ''.join([prepared_text, token.text.lower(), ' '])\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5137de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_russian_text(input_file, output_file):\n",
    "\n",
    "    segmenter = Segmenter()\n",
    "    emb = NewsEmbedding()\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    ner_tagger = NewsNERTagger(emb)\n",
    "    morph_vocab = MorphVocab()\n",
    "    \n",
    "    label_dict = {'NUM': '0', 'PRON': '1', 'PER': '2', 'LOC': '3', 'ORG': '4'}\n",
    "    next_label_num = 5\n",
    "\n",
    "    with open(input_file, encoding = \"utf-8\") as fin:\n",
    "        raw_text = ' '.join(fin.readlines())\n",
    "\n",
    "    raw_text = re.sub(r'\\d+', '0' , raw_text)\n",
    "\n",
    "    doc = Doc(raw_text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "\n",
    "    for span in reversed(doc.ner.spans):\n",
    "        if span.type not in label_dict:\n",
    "            label_dict[span.type] = str(next_label_num)\n",
    "            next_label_num += 1\n",
    "        raw_text = \"\".join((raw_text[:span.start], label_dict[span.type], raw_text[span.stop:]))\n",
    "\n",
    "    doc = Doc(raw_text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "\n",
    "    prepared_text = ''\n",
    "    prev_num = False\n",
    "    for token in doc.tokens:\n",
    "        if token.pos == 'NUM' and not token.text.isdigit():\n",
    "            if not prev_num:\n",
    "                prepared_text += '0'\n",
    "                prepared_text += ' '\n",
    "                prev_num = True\n",
    "            continue\n",
    "\n",
    "        prev_num = False\n",
    "\n",
    "        if token.pos in label_dict:\n",
    "            prepared_text += label_dict[token.pos]\n",
    "            prepared_text += ' '\n",
    "\n",
    "        elif token.pos != 'PUNCT':\n",
    "            try:\n",
    "                token.lemmatize(morph_vocab)\n",
    "                prepared_text += token.lemma.lower()\n",
    "                prepared_text += ' '\n",
    "            except Exception as ex:\n",
    "                prepared_text += token.text.lower()\n",
    "                prepared_text += ' '\n",
    "    with open(output_file, 'w', encoding = \"utf-8\") as fout:\n",
    "       fout.write(prepared_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8416ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_russian_text('pure_literature.txt', 'lemma_literature.txt')\n",
    "prepare_russian_text('pure_anekdots.txt', 'lemma_anekdots_truncated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38beba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "anekdots_splitted = [i for i in open('pure_anekdots.txt', 'r', encoding = 'utf-8').read().split('\\n\\n') if i != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653e9844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- Как водичка ?\\n- А я здесь как женшина сижу, а не как термометр.', '- Я затрудняюсь поставить вам диагноз ... Наверное, это алкоголизм.\\n- Хорошо, доктор. Я приду, когда Вы будете трезвым.', '- Что такое дефицит в маркистском понимании?\\n- Это объективная реальность, не данная нам в ощущениях.\\n- Это вы в идеалистическом понимании, а в практическом?\\n- Объективная реальность данная в ощущениях, но не нам.', '- Можно у вас срочно отремонтировать часы?\\n- Нет\\n- Что такое\\n- Нет\\n- А что здесь делают?\\n- Здесь делают обрезание\\n- Тогда какого черта вы повесили циферблат над входом?\\n- А что б вы хотели, чтоб мы там повесили?', '- Из-за тебя я проиграл уйму денег!\\n- Почему ты не заговорил?\\n- Чудак!\\n- Ты только представь, сколько денег мы загребем завтра.', '- Входите,\\n- Через 15 минут вам на операцию. А пока отдыхайте.\\n- Сестра!\\n- Не обьясните мне, почему вы стучали в дверь перед тем, как войти???']\n"
     ]
    }
   ],
   "source": [
    "print(anekdots_splitted[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96021765",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_russian_text('all_jokes_files/TEST_JOKES.txt', 'lemma_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8022ad47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepare_russian_text('all_jokes_files/dataset.txt', 'lemma_jokes_big1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f54477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_russian_text('all_jokes_files/extract_anekdots.txt', 'lemma_jokes_big2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592f30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_russian_text('all_jokes_files/jokes.txt', 'lemma_jokes_big3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58e88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_russian_text('all_jokes_files/jokes_2.txt', 'lemma_jokes_big4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc60ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661f4b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af10a3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/53/29/d4ba96e8c3032f799f778a83356c4956dc5b99cd72d1300704d71e129879/spacy-3.8.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.8.5-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/de/30/ceb9217cdba72bc0bf8466e373e12e5a42945cc85eda0a7c479e319e07ae/murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/56/c8/75f75889401b20f4c3a7c5965dda09df42913e904ddc2ffe7ef3bdf25061/cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e4/fc/78cdbdb79f5d6d45949e72c32445d6c060977ad50a1dcfc0392622165f7c/preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.4.0,>=8.3.4 from https://files.pythonhosted.org/packages/d9/98/f910b8d8113ab9b955a68e9bbf0d5bd0e828f22dd6d3c226af6ec3970817/thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/06/7c/34330a89da55610daa5f245ddce5aab81244321101614751e7537f125133/wasabi-1.1.3-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/bb/da/657a685f63028dcb00ccdc4ac125ed347c8bff6fa0dab6a9eb3dc45f3223/srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.5.0,>=0.1.0 from https://files.pythonhosted.org/packages/2a/87/abd57374044e1f627f0a905ac33c1a7daab35a3a815abfea4e1bafd3fdb1/weasel-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Obtaining dependency information for typer<1.0.0,>=0.3.0 from https://files.pythonhosted.org/packages/7f/fc/5b29fea8cee020515ca82cc68e3b8e1e34bb19a3535ad854cac9257b414c/typer-0.15.2-py3-none-any.whl.metadata\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/c3/6b/068c2ea7a712bf805c62445bd9e9c06d7340358ef2824150eceac027444b/langcodes-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for language-data>=1.2 from https://files.pythonhosted.org/packages/5d/e9/5a5ffd9b286db82be70d677d0a91e4d58f7912bb8dd026ddeeb4abe70679/language_data-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for blis<1.3.0,>=1.2.0 from https://files.pythonhosted.org/packages/7c/c0/047fef3ac4a531903c52ba7c108fd608556627723bfef7554f040b10e556/blis-1.2.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading blis-1.2.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/0c/00/3106b1854b45bd0474ced037dfe6b73b90fe68a68968cef47c23de3d43d2/confection-0.1.5-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for shellingham>=1.3.0 from https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for rich>=10.11.0 from https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<1.0.0,>=0.7.0 from https://files.pythonhosted.org/packages/e8/0f/b1a9b09a84ef98b9fc38d50c6b2815cb2256b804a78e7d838ddfbdc035c7/cloudpathlib-0.21.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for marisa-trie>=1.1.0 from https://files.pythonhosted.org/packages/fc/98/574b4e143e0a2f5f71af8716b6c4a8a46220f75a6e0847ce7d11ee0ba4aa/marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.5-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.2 MB 2.4 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/12.2 MB 4.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/12.2 MB 5.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.9/12.2 MB 9.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.2 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.9/12.2 MB 23.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.2/12.2 MB 18.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 23.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.8/12.2 MB 22.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 19.3 MB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 183.0/183.0 kB 11.5 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB ? eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 47.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.1/45.1 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading blis-1.2.1-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 3.1/6.2 MB 66.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 79.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 56.6 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.7/52.7 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 4.3/5.4 MB 91.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 57.2 MB/s eta 0:00:00\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "   ---------------------------------------- 0.0/243.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 243.2/243.2 kB 14.6 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.0/152.0 kB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, rich, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 rich-14.0.0 shellingham-1.5.4 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/400.7 MB 330.3 kB/s eta 0:20:13\n",
      "     -------------------------------------- 0.0/400.7 MB 487.6 kB/s eta 0:13:42\n",
      "     ---------------------------------------- 0.2/400.7 MB 1.1 MB/s eta 0:05:50\n",
      "     ---------------------------------------- 0.4/400.7 MB 2.4 MB/s eta 0:02:49\n",
      "     ---------------------------------------- 0.7/400.7 MB 3.4 MB/s eta 0:01:59\n",
      "     ---------------------------------------- 1.7/400.7 MB 6.3 MB/s eta 0:01:04\n",
      "     --------------------------------------- 4.4/400.7 MB 13.9 MB/s eta 0:00:29\n",
      "      -------------------------------------- 7.0/400.7 MB 19.5 MB/s eta 0:00:21\n",
      "      -------------------------------------- 7.6/400.7 MB 19.3 MB/s eta 0:00:21\n",
      "      -------------------------------------- 8.0/400.7 MB 17.1 MB/s eta 0:00:23\n",
      "      -------------------------------------- 9.2/400.7 MB 18.3 MB/s eta 0:00:22\n",
      "      -------------------------------------- 9.8/400.7 MB 17.9 MB/s eta 0:00:22\n",
      "      ------------------------------------- 10.3/400.7 MB 19.9 MB/s eta 0:00:20\n",
      "     - ------------------------------------ 10.7/400.7 MB 21.9 MB/s eta 0:00:18\n",
      "     - ------------------------------------ 11.9/400.7 MB 25.2 MB/s eta 0:00:16\n",
      "     - ------------------------------------ 13.2/400.7 MB 22.6 MB/s eta 0:00:18\n",
      "     - ------------------------------------ 14.0/400.7 MB 21.9 MB/s eta 0:00:18\n",
      "     - ------------------------------------ 15.0/400.7 MB 20.5 MB/s eta 0:00:19\n",
      "     - ------------------------------------ 16.1/400.7 MB 19.8 MB/s eta 0:00:20\n",
      "     - ------------------------------------ 18.0/400.7 MB 19.8 MB/s eta 0:00:20\n",
      "     -- ----------------------------------- 22.4/400.7 MB 36.3 MB/s eta 0:00:11\n",
      "     -- ----------------------------------- 26.9/400.7 MB 73.1 MB/s eta 0:00:06\n",
      "     -- ----------------------------------- 31.0/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 33.8/400.7 MB 81.8 MB/s eta 0:00:05\n",
      "     --- ---------------------------------- 38.3/400.7 MB 81.8 MB/s eta 0:00:05\n",
      "     ---- --------------------------------- 42.7/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 47.0/400.7 MB 93.0 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 51.3/400.7 MB 93.0 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 55.0/400.7 MB 93.0 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 59.8/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 63.9/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 68.6/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     ------ ------------------------------ 73.3/400.7 MB 108.8 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 77.2/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 81.7/400.7 MB 93.0 MB/s eta 0:00:04\n",
      "     -------- ----------------------------- 85.9/400.7 MB 93.0 MB/s eta 0:00:04\n",
      "     -------- ----------------------------- 90.4/400.7 MB 93.0 MB/s eta 0:00:04\n",
      "     -------- ----------------------------- 94.4/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     --------- ---------------------------- 98.6/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     --------- --------------------------- 103.0/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     --------- --------------------------- 107.2/400.7 MB 93.9 MB/s eta 0:00:04\n",
      "     ---------- -------------------------- 111.4/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ---------- -------------------------- 115.5/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ----------- ------------------------- 119.8/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ----------- ------------------------- 124.7/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ----------- ------------------------- 129.1/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ----------- ------------------------ 133.2/400.7 MB 110.0 MB/s eta 0:00:03\n",
      "     ------------ ------------------------ 136.9/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ------------- ----------------------- 141.6/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ------------- ----------------------- 146.2/400.7 MB 81.8 MB/s eta 0:00:04\n",
      "     ------------- ----------------------- 150.5/400.7 MB 93.9 MB/s eta 0:00:03\n",
      "     -------------- ---------------------- 154.7/400.7 MB 93.9 MB/s eta 0:00:03\n",
      "     -------------- ---------------------- 158.7/400.7 MB 93.9 MB/s eta 0:00:03\n",
      "     --------------- --------------------- 163.3/400.7 MB 81.8 MB/s eta 0:00:03\n",
      "     --------------- --------------------- 168.4/400.7 MB 93.9 MB/s eta 0:00:03\n",
      "     --------------- --------------------- 172.7/400.7 MB 93.0 MB/s eta 0:00:03\n",
      "     ---------------- -------------------- 177.5/400.7 MB 93.0 MB/s eta 0:00:03\n",
      "     ---------------- -------------------- 180.7/400.7 MB 81.8 MB/s eta 0:00:03\n",
      "     ----------------- ------------------- 185.6/400.7 MB 81.8 MB/s eta 0:00:03\n",
      "     ----------------- ------------------- 189.4/400.7 MB 93.9 MB/s eta 0:00:03\n",
      "     ----------------- ------------------- 194.2/400.7 MB 93.9 MB/s eta 0:00:03\n",
      "     ----------------- ------------------ 199.2/400.7 MB 108.8 MB/s eta 0:00:02\n",
      "     ------------------ ------------------ 203.0/400.7 MB 93.9 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 207.3/400.7 MB 93.0 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 211.1/400.7 MB 93.0 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 215.9/400.7 MB 93.0 MB/s eta 0:00:02\n",
      "     ------------------- ---------------- 220.2/400.7 MB 110.0 MB/s eta 0:00:02\n",
      "     -------------------- ---------------- 224.3/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     --------------------- --------------- 229.3/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     --------------------- --------------- 232.9/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     --------------------- --------------- 237.8/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     ---------------------- -------------- 241.2/400.7 MB 93.0 MB/s eta 0:00:02\n",
      "     ---------------------- -------------- 245.0/400.7 MB 93.0 MB/s eta 0:00:02\n",
      "     ----------------------- ------------- 249.5/400.7 MB 93.0 MB/s eta 0:00:02\n",
      "     ---------------------- ------------- 254.4/400.7 MB 110.0 MB/s eta 0:00:02\n",
      "     ----------------------- ------------- 259.2/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 264.3/400.7 MB 93.0 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 269.0/400.7 MB 93.0 MB/s eta 0:00:02\n",
      "     ------------------------ ----------- 273.7/400.7 MB 108.8 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 278.5/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 283.5/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 288.5/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 293.3/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 298.3/400.7 MB 93.9 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 303.2/400.7 MB 93.0 MB/s eta 0:00:02\n",
      "     --------------------------- -------- 308.0/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ------- 312.9/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ------- 317.8/400.7 MB 110.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 322.6/400.7 MB 93.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ------ 327.6/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ------ 332.5/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------------ ----- 337.5/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------------ ----- 342.4/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ---- 347.3/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ---- 352.3/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------------------- --- 357.4/400.7 MB 110.0 MB/s eta 0:00:01\n",
      "     -------------------------------- --- 362.3/400.7 MB 110.0 MB/s eta 0:00:01\n",
      "     -------------------------------- --- 367.0/400.7 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 367.5/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 370.8/400.7 MB 73.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 375.4/400.7 MB 59.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 380.3/400.7 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 383.8/400.7 MB 93.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 387.5/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  391.0/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  394.3/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  397.9/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 72.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 400.7/400.7 MB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b61416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import glob\n",
    "import spacy\n",
    "spacy.load('en_core_web_lg')\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b68c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is for decontracting shortened words (won't -> will not)\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won['’‘`]t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can['’‘`]t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"ain['’‘`]t\", \"am not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n['’‘`]t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]m\", \" am\", phrase)\n",
    "\n",
    "    #phrase = re.sub('([.;!?])', r' \\1 ', phrase)\n",
    "    phrase = re.sub(r'[^\\w.?!;]', ' ', phrase)\n",
    "    phrase = re.sub(' +', ' ', phrase)\n",
    "    sentences = re.split('([.;!?] *)', phrase)\n",
    "\n",
    "    return ' '.join([i.capitalize() for i in  sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5157cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_english_text(nlp, input_file, output_file):\n",
    "    '''\n",
    "    input_file - it is path to the EXISTING txt file. It shuold contain\n",
    "    raw text in English\n",
    "\n",
    "    output_file - it is path to txt file, where the preprocessed text will be written\n",
    "\n",
    "    '''\n",
    "    pos_dict = {'PROPN': 'person1', 'PRON': 'pron1', 'NUM': 'ordinal1'}\n",
    "    fin  = open(input_file, 'r', encoding = 'utf-8')\n",
    "    with open(output_file, 'w', encoding = 'utf-8') as prepared_text:\n",
    "        for line in fin:\n",
    "            preprocessed_text = decontracted(line.strip())\n",
    "            nlp_doc = nlp(preprocessed_text)\n",
    "            for token in nlp_doc:\n",
    "                if token.pos_ in pos_dict:\n",
    "                    prepared_text.write(pos_dict[token.pos_])\n",
    "                    prepared_text.write(' ')\n",
    "                elif token.lemma_.isdigit():\n",
    "                    prepared_text.write('ordinal1')\n",
    "                    prepared_text.write(' ')\n",
    "                elif token.pos_ != 'PUNCT':\n",
    "                    prepared_text.write(token.lemma_.lower())\n",
    "                    prepared_text.write(' ')\n",
    "            prepared_text.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313cc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "prepare_english_text(nlp, 'pure_eng_jokes.txt', 'lemma_eng_jokes.txt')\n",
    "prepare_english_text(nlp, 'eng_literature.txt', 'lemma_eng_liter.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cfcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
