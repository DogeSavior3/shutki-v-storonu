{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905ca47c-6e6c-430c-80cc-57cc4deb5ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49579dab-3ba8-4ddc-8497-9e8120084d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 09:01:36 INFO: Loading these models for language: hbo (Ancient_Hebrew):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | ptnk          |\n",
      "| mwt       | ptnk          |\n",
      "| pos       | ptnk_nocharlm |\n",
      "| lemma     | ptnk_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-03-13 09:01:36 WARNING: GPU requested, but is not available!\n",
      "2025-03-13 09:01:36 INFO: Using device: cpu\n",
      "2025-03-13 09:01:36 INFO: Loading: tokenize\n",
      "2025-03-13 09:01:36 INFO: Loading: mwt\n",
      "2025-03-13 09:01:36 INFO: Loading: pos\n",
      "2025-03-13 09:01:36 INFO: Loading: lemma\n",
      "2025-03-13 09:01:36 INFO: Done loading processors!\n",
      "2025-03-13 09:01:36 INFO: Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-03-13 09:01:36 WARNING: GPU requested, but is not available!\n",
      "2025-03-13 09:01:36 INFO: Using device: cpu\n",
      "2025-03-13 09:01:36 INFO: Loading: tokenize\n",
      "2025-03-13 09:01:36 INFO: Loading: mwt\n",
      "2025-03-13 09:01:36 INFO: Loading: pos\n",
      "2025-03-13 09:01:37 INFO: Loading: lemma\n",
      "2025-03-13 09:01:37 INFO: Done loading processors!\n",
      "2025-03-13 09:01:37 INFO: Loading these models for language: he (Hebrew):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | iahltwiki          |\n",
      "| mwt       | iahltwiki          |\n",
      "| pos       | iahltwiki_charlm   |\n",
      "| lemma     | iahltwiki_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-03-13 09:01:37 WARNING: GPU requested, but is not available!\n",
      "2025-03-13 09:01:37 INFO: Using device: cpu\n",
      "2025-03-13 09:01:37 INFO: Loading: tokenize\n",
      "2025-03-13 09:01:37 INFO: Loading: mwt\n",
      "2025-03-13 09:01:37 INFO: Loading: pos\n",
      "2025-03-13 09:01:37 INFO: Loading: lemma\n",
      "2025-03-13 09:01:37 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "input_folder = 'example'\n",
    "output_folders = {\n",
    "    'ancient-hebrew': 'lemma_stanza_anc',\n",
    "    'hebrew-1': 'lemma_stanza_HTB',\n",
    "    'hebrew-2': 'lemma_stanza_IAHLT',\n",
    "    'all': 'lemma_stanza_all'\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'ancient-hebrew': stanza.Pipeline(\n",
    "        lang = 'hbo',\n",
    "        processors = 'tokenize, mwt, pos, lemma',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    ),\n",
    "    'hebrew-1': stanza.Pipeline(\n",
    "        lang = 'he',\n",
    "        processors = 'tokenize, mwt, pos, lemma',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    ),\n",
    "    'hebrew-2': stanza.Pipeline(\n",
    "        lang = 'he',\n",
    "        processors = 'tokenize, mwt, pos, lemma',\n",
    "        package = 'iahltwiki',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "939b7fe0-fb4e-46fa-9d28-6142b74554de",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'PROPN': 'person1', 'PRON': 'pron1', 'NUM': 'ordinal1'}\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^\\u0590-\\u05FF\\u0030-\\u0039\\s\\.,:!?\\'\"«»\\(\\)\\[\\]\\-]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def process_text_with_stanza(text, pipeline):\n",
    "    processed_text = pipeline(text)\n",
    "    prepared_text = []\n",
    "    \n",
    "    for sentence in processed_text.sentences:\n",
    "        for word in sentence.words:\n",
    "            upos = word.upos\n",
    "            \n",
    "            if upos is None:\n",
    "                continue\n",
    "            \n",
    "            if upos in label_dict:\n",
    "                prepared_text.append(label_dict[upos])\n",
    "            elif upos != 'PUNCT':\n",
    "                lemma = word.lemma.lower() if word.lemma else word.text.lower()\n",
    "                prepared_text.append(lemma)\n",
    "    \n",
    "    return ' '.join(prepared_text)\n",
    "\n",
    "def split_processed_text(processed_text, separator):\n",
    "    return processed_text.split(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2942bb-b769-4a7d-92d5-b879127ad71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        text = clean_text(text)\n",
    "        \n",
    "        for lang_code, pipeline in pipelines.items():\n",
    "            processed_text = process_text_with_stanza(text, pipeline)\n",
    "            \n",
    "            output_path = os.path.join(output_folders[lang_code], filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(processed_text)\n",
    "        \n",
    "        combined_text = text\n",
    "        for pipeline in pipelines.values():\n",
    "            combined_text = process_text_with_stanza(combined_text, pipeline)\n",
    "        \n",
    "        combined_output_path = os.path.join(output_folders['all'], filename)\n",
    "        with open(combined_output_path, 'w', encoding='utf-8') as combined_output_file:\n",
    "            combined_output_file.write(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b71adadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_batches(input_folder, output_folders, pipelines, batch_size=8):\n",
    "    texts_batch = []\n",
    "    filenames_batch = []\n",
    "\n",
    "    for filename in sorted(os.listdir(input_folder)):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().strip()\n",
    "\n",
    "        cleaned_text = clean_text(text)\n",
    "        texts_batch.append(cleaned_text)\n",
    "        filenames_batch.append(filename)\n",
    "\n",
    "        if len(texts_batch) >= batch_size:\n",
    "            process_batch_and_save(texts_batch, filenames_batch, output_folders, pipelines)\n",
    "            texts_batch.clear()\n",
    "            filenames_batch.clear()\n",
    "\n",
    "    if texts_batch:\n",
    "        process_batch_and_save(texts_batch, filenames_batch, output_folders, pipelines)\n",
    "\n",
    "def process_batch_and_save(texts_batch, filenames_batch, output_folders, pipelines):\n",
    "    for lang_code, pipeline in pipelines.items():\n",
    "        print(f\"Processing batch for {lang_code}\")\n",
    "        processed_texts = process_batch(texts_batch, pipeline)\n",
    "\n",
    "        for i, processed_text in enumerate(processed_texts):\n",
    "            output_path = os.path.join(output_folders[lang_code], filenames_batch[i])\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(processed_text + '\\n')\n",
    "\n",
    "def process_batch(batch_texts, pipeline):\n",
    "    combined_text = ' <<br>> '.join(batch_texts)\n",
    "    processed_doc = pipeline(combined_text)\n",
    "    \n",
    "    results = []\n",
    "    current_result = []\n",
    "    \n",
    "    for sentence in processed_doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.text == '<<br>>':\n",
    "                results.append(' '.join(current_result))\n",
    "                current_result = []\n",
    "                continue\n",
    "                \n",
    "            upos = word.upos\n",
    "            if upos is None:\n",
    "                continue\n",
    "            \n",
    "            if upos in label_dict:\n",
    "                current_result.append(label_dict[upos])\n",
    "            elif upos != 'PUNCT':\n",
    "                lemma = word.lemma.lower() if word.lemma else word.text.lower()\n",
    "                current_result.append(lemma)\n",
    "    \n",
    "    if current_result:\n",
    "        results.append(' '.join(current_result))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89a5a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch for ancient-hebrew\n",
      "Processing batch for hebrew-1\n",
      "Processing batch for hebrew-2\n"
     ]
    }
   ],
   "source": [
    "process_files_in_batches(input_folder, output_folders, pipelines, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1d113c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 11:15:41 INFO: Loading these models for language: hbo (Ancient_Hebrew):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | ptnk          |\n",
      "| mwt       | ptnk          |\n",
      "| pos       | ptnk_nocharlm |\n",
      "| lemma     | ptnk_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-03-13 11:15:41 WARNING: GPU requested, but is not available!\n",
      "2025-03-13 11:15:41 INFO: Using device: cpu\n",
      "2025-03-13 11:15:41 INFO: Loading: tokenize\n",
      "2025-03-13 11:15:41 INFO: Loading: mwt\n",
      "2025-03-13 11:15:41 INFO: Loading: pos\n",
      "2025-03-13 11:15:42 INFO: Loading: lemma\n",
      "2025-03-13 11:15:42 INFO: Done loading processors!\n",
      "2025-03-13 11:15:42 INFO: Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-03-13 11:15:42 WARNING: GPU requested, but is not available!\n",
      "2025-03-13 11:15:42 INFO: Using device: cpu\n",
      "2025-03-13 11:15:42 INFO: Loading: tokenize\n",
      "2025-03-13 11:15:42 INFO: Loading: mwt\n",
      "2025-03-13 11:15:42 INFO: Loading: pos\n",
      "2025-03-13 11:15:42 INFO: Loading: lemma\n",
      "2025-03-13 11:15:42 INFO: Done loading processors!\n",
      "2025-03-13 11:15:42 INFO: Loading these models for language: he (Hebrew):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | iahltwiki          |\n",
      "| mwt       | iahltwiki          |\n",
      "| pos       | iahltwiki_charlm   |\n",
      "| lemma     | iahltwiki_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-03-13 11:15:42 WARNING: GPU requested, but is not available!\n",
      "2025-03-13 11:15:42 INFO: Using device: cpu\n",
      "2025-03-13 11:15:42 INFO: Loading: tokenize\n",
      "2025-03-13 11:15:42 INFO: Loading: mwt\n",
      "2025-03-13 11:15:42 INFO: Loading: pos\n",
      "2025-03-13 11:15:42 INFO: Loading: lemma\n",
      "2025-03-13 11:15:42 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "input_folder = 'example'\n",
    "output_folders = {\n",
    "    'ancient-hebrew': 'lemma_stanza_anc',\n",
    "    'hebrew-1': 'lemma_stanza_HTB',\n",
    "    'hebrew-2': 'lemma_stanza_IAHLT',\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'ancient-hebrew': stanza.Pipeline(\n",
    "        lang='hbo',\n",
    "        processors='tokenize,mwt,pos,lemma',\n",
    "        model_dir='stanza_resources',\n",
    "        download_method=None,\n",
    "        use_gpu=True\n",
    "    ),\n",
    "    'hebrew-1': stanza.Pipeline(\n",
    "        lang='he',\n",
    "        processors='tokenize,mwt,pos,lemma',\n",
    "        model_dir='stanza_resources',\n",
    "        download_method=None,\n",
    "        use_gpu=True\n",
    "    ),\n",
    "    'hebrew-2': stanza.Pipeline(\n",
    "        lang='he',\n",
    "        processors='tokenize,mwt,pos,lemma',\n",
    "        package='iahltwiki',\n",
    "        model_dir='stanza_resources',\n",
    "        download_method=None,\n",
    "        use_gpu=True\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ce44e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch for ancient-hebrew\n",
      "Processing batch for hebrew-1\n",
      "Processing batch for hebrew-2\n"
     ]
    }
   ],
   "source": [
    "label_dict = {'PROPN': 'person1', 'PRON': 'pron1', 'NUM': 'ordinal1'}\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\u0590-\\u05FF\\u0030-\\u0039\\s\\.,:!?\\'\"«»\\(\\)\\[\\]\\-]', '', text)\n",
    "\n",
    "\n",
    "def process_documents(documents, pipeline):\n",
    "    in_docs = [stanza.Document([], text=doc) for doc in documents]\n",
    "    out_docs = pipeline(in_docs)\n",
    "\n",
    "    results = []\n",
    "    for doc in out_docs:\n",
    "        current_result = []\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                upos = word.upos\n",
    "                if upos is None:\n",
    "                    continue\n",
    "\n",
    "                if upos in label_dict:\n",
    "                    current_result.append(label_dict[upos])\n",
    "                elif upos != 'PUNCT':\n",
    "                    lemma = word.lemma.lower() if word.lemma else word.text.lower()\n",
    "                    current_result.append(lemma)\n",
    "        results.append(' '.join(current_result))\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_files_in_batches(input_folder, output_folders, pipelines, max_texts=500):\n",
    "    texts_batch = []\n",
    "    filenames_batch = []\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().strip()\n",
    "\n",
    "        cleaned_text = clean_text(text)\n",
    "        texts_batch.append(cleaned_text)\n",
    "        filenames_batch.append(filename)\n",
    "\n",
    "        if len(texts_batch) >= max_texts:\n",
    "            process_and_save(texts_batch, filenames_batch, output_folders, pipelines)\n",
    "            texts_batch.clear()\n",
    "            filenames_batch.clear()\n",
    "\n",
    "    if texts_batch:\n",
    "        process_and_save(texts_batch, filenames_batch, output_folders, pipelines)\n",
    "\n",
    "\n",
    "def process_and_save(texts_batch, filenames_batch, output_folders, pipelines):\n",
    "    for lang_code, pipeline in pipelines.items():\n",
    "        print(f\"Processing batch for {lang_code}\")\n",
    "        processed_texts = process_documents(texts_batch, pipeline)\n",
    "\n",
    "        for i, processed_text in enumerate(processed_texts):\n",
    "            output_path = os.path.join(output_folders[lang_code], filenames_batch[i])\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(processed_text + '\\n')\n",
    "\n",
    "process_files_in_batches(input_folder, output_folders, pipelines, max_texts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e6970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
