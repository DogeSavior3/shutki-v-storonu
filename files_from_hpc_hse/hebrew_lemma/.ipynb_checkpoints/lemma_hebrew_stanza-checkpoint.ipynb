{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905ca47c-6e6c-430c-80cc-57cc4deb5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source supercomputer_files/hebrew/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f241108-19b6-4693-bd8a-a112987894b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 24.3.1 from /opt/software/python/envs/google_colab_gpu_2024/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -v install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ee26c6-4d32-41e2-8af2-2d2060e43400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mkorol/hebrew_lemma\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9e14286-3603-4285-99f6-83697c7d5dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_hebrew_stanza.ipynb  \u001b[0m\u001b[01;34mstanza_resources\u001b[0m/  \u001b[01;34msupercomputer_files\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49579dab-3ba8-4ddc-8497-9e8120084d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[1;32m      5\u001b[0m input_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupercomputer_files/pure\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m output_folders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mancient-hebrew\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_stanza_anc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhebrew-1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_stanza_HTB\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhebrew-2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_stanza_IAHLT\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_stanza_all\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m }\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "input_folder = 'supercomputer_files/pure'\n",
    "output_folders = {\n",
    "    'ancient-hebrew': 'lemma_stanza_anc',\n",
    "    'hebrew-1': 'lemma_stanza_HTB',\n",
    "    'hebrew-2': 'lemma_stanza_IAHLT',\n",
    "    'all': 'lemma_stanza_all'\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'ancient-hebrew': stanza.Pipeline(\n",
    "        lang = 'hbo',\n",
    "        processors = 'tokenize, mwt, pos, lemma',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    ),\n",
    "    'hebrew-1': stanza.Pipeline(\n",
    "        lang = 'he',\n",
    "        processors = 'tokenize, mwt, pos, lemma',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    ),\n",
    "    'hebrew-2': stanza.Pipeline(\n",
    "        lang = 'he',\n",
    "        processors = 'tokenize, mwt, pos, lemma',\n",
    "        package = 'iahltwiki',\n",
    "        model_dir = 'stanza_resources',\n",
    "        download_method = None,\n",
    "        use_gpu = True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "939b7fe0-fb4e-46fa-9d28-6142b74554de",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'PROPN': 'person1', 'PRON': 'pron1', 'NUM': 'ordinal1'}\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^\\u0590-\\u05FF\\u0030-\\u0039\\s\\.,:!?\\'\"«»\\(\\)\\[\\]\\-]', '', text)\n",
    "    return text\n",
    "\n",
    "def process_text_with_stanza(text, pipeline):\n",
    "    processed_text = pipeline(text)\n",
    "    prepared_text = []\n",
    "    \n",
    "    for sentence in processed_text.sentences:\n",
    "        for word in sentence.words:\n",
    "            upos = word.upos\n",
    "            \n",
    "            if upos is None:\n",
    "                continue\n",
    "            \n",
    "            if upos in label_dict:\n",
    "                prepared_text.append(label_dict[upos])\n",
    "            elif upos != 'PUNCT':\n",
    "                lemma = word.lemma.lower() if word.lemma else word.text.lower()\n",
    "                prepared_text.append(lemma)\n",
    "    \n",
    "    return ' '.join(prepared_text)\n",
    "\n",
    "def split_processed_text(processed_text, separator):\n",
    "    return processed_text.split(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2942bb-b769-4a7d-92d5-b879127ad71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        text = clean_text(text)\n",
    "        \n",
    "        for lang_code, pipeline in pipelines.items():\n",
    "            processed_text = process_text_with_stanza(text, pipeline)\n",
    "            \n",
    "            output_path = os.path.join(output_folders[lang_code], filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(processed_text)\n",
    "        \n",
    "        combined_text = text\n",
    "        for pipeline in pipelines.values():\n",
    "            combined_text = process_text_with_stanza(combined_text, pipeline)\n",
    "        \n",
    "        combined_output_path = os.path.join(output_folders['all'], filename)\n",
    "        with open(combined_output_path, 'w', encoding='utf-8') as combined_output_file:\n",
    "            combined_output_file.write(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0a285af",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = {}\n",
    "file_names = {}\n",
    "buff_limit = 1024 * 1024 * 10\n",
    "separator = '<<br>>'\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        cleaned_text = clean_text(text)\n",
    "\n",
    "        if filename not in buffer:\n",
    "            buffer[filename] = []\n",
    "            file_names[filename] = []\n",
    "\n",
    "        buffer[filename].append(cleaned_text)\n",
    "        file_names[filename].append(filename)\n",
    "\n",
    "        buffer_size = sum(len(t) for t in buffer[filename])\n",
    "        if buffer_size >= buff_limit:\n",
    "            combined_text = separator.join(buffer[filename])\n",
    "\n",
    "            for lang_code, pipeline in pipelines.items():\n",
    "                processed_text = process_text_with_stanza(combined_text, pipeline)\n",
    "\n",
    "                processed_texts = split_processed_text(processed_text, separator)\n",
    "\n",
    "                for original_filename, processed_part in zip(file_names[filename], processed_texts):\n",
    "                    output_path = os.path.join(output_folders[lang_code], original_filename)\n",
    "                    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                        output_file.write(processed_part)\n",
    "\n",
    "            buffer[filename] = []\n",
    "            file_names[filename] = []\n",
    "\n",
    "for filename, texts in buffer.items():\n",
    "    if texts:\n",
    "        combined_text = separator.join(texts)\n",
    "\n",
    "        for lang_code, pipeline in pipelines.items():\n",
    "            processed_text = process_text_with_stanza(combined_text, pipeline)\n",
    "            processed_texts = split_processed_text(processed_text, separator)\n",
    "\n",
    "            for original_filename, processed_part in zip(file_names[filename], processed_texts):\n",
    "                output_path = os.path.join(output_folders[lang_code], original_filename)\n",
    "                with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(processed_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f4a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4 CUDA 12.4 [python-pytorch2_4]",
   "language": "python",
   "name": "conda-env-python-pytorch2_4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
